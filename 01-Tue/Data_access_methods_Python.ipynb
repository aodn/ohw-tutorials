{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b743e8-ca81-4388-99bd-79f92eeac434",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data Access Methods\n",
    "\n",
    "This tutorial demostrates several ways data can be accessed remotely and loaded into a Python environment, including\n",
    "\n",
    "* THREDDS/OPeNDAP\n",
    "* OGC Web Feature Service (WFS)\n",
    "* direct access to files on cloud storage (AWS S3)\n",
    "* cloud-optimised formats Zarr & Parquet\n",
    "* New OGC APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96368746-0a9e-4b81-ae32-08823e379d67",
   "metadata": {},
   "source": [
    "## Old school\n",
    "\n",
    "The old (and still common) way to access data is to first download it to your computer and read it from there. \n",
    "This is easy for small datasets, but not always ideal:\n",
    "* What if the data is bigger than your hard disk?\n",
    "* What if you only need a small fraction of a dataset?\n",
    "* What if the dataset is routinely updated and you want to re-run your analysis on the latest data?\n",
    "* What if you want to run your analysis on another computer or in the cloud?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4da0-e672-4ef1-beb0-b73db5eb5a6a",
   "metadata": {},
   "source": [
    "These days it is often more convenient to have data managed in a central location and access it remotely.\n",
    "There are many ways this can be done. In this tutorial we will look at a few of the common ones, and some of the newer ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf83805-6e87-4fe4-95b2-1deabfab4380",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "TODO"
    ]
   },
   "source": [
    "## OPeNDAP\n",
    "\n",
    "* [OPeNDAP](https://www.opendap.org/about) stands for \"Open-source Project for a Network Data Access Protocol\"\n",
    "* Provices access to metadata and data subsets via the Web without downloading an entire dataset\n",
    "* Many tools that can read NetCDF files can also talk to an OPeNDAP URL directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfee5a9-28f4-4ef0-b471-de04a84e1333",
   "metadata": {},
   "source": [
    "In Python, we can simply open the URL with `xarray`, then proceed with our analysis using the resulgting `Dataset` object.\n",
    "\n",
    "Here we use an example from the [AODN THREDDS server](https://thredds.aodn.org.au/thredds/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118639e-503d-4552-8b32-9ce5cc33535a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "opendap_url = \"https://thredds.aodn.org.au/thredds/dodsC/IMOS/ANMN/NSW/PH100/gridded_timeseries/IMOS_ANMN-NSW_TZ_20091029_PH100_FV02_TEMP-gridded-timeseries_END-20230316_C-20230520.nc\"\n",
    "\n",
    "ds_mooring = xr.open_dataset(opendap_url)\n",
    "ds_mooring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba40af7-cf45-4a09-a0d1-fadd5ec21fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_mooring.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68891c3-0532-4e96-b4c8-cad9883b64cf",
   "metadata": {},
   "source": [
    "This dataset is derived from repeated deployments of moored temperature loggets, binned to hourly intervals and interpolated to a fixed set of target depths. See the file metadata, or the associated [metadata record](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/279a50e3-21a5-4590-85a0-71f963efab82) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a789a-81e7-4725-a5e1-b124eda515df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "\n",
    "# The full dataset can be plotted like this:\n",
    "# ds_mooring.hvplot.scatter(x=\"TIME\", y=\"DEPTH\", c=\"TEMP\", cmap=\"coolwarm\", alpha=0.2, flip_yaxis=True)\n",
    "\n",
    "# Hourly averages x 12 depths x 13+ yr = over a million points to plot!\n",
    "# Let's just look at a year's worth to speed things up...\n",
    "ds_mooring.sel(TIME=\"2022\").hvplot.scatter(x=\"TIME\", y=\"DEPTH\", c=\"TEMP\", cmap=\"coolwarm\", alpha=0.2, flip_yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb6d7d-3110-459c-822f-1d613f883128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... or we can look at the full timeseries of temperature at a single depth\n",
    "ds_mooring.TEMP.sel(DEPTH=30).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571909e3-71b4-427d-aefb-3d64dbd6158c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Web Feature Service (WFS)\n",
    "\n",
    "* A [standard](http://www.opengeospatial.org/standards/wfs) of the [Open Geospatial Consortium](http://www.opengeospatial.org/) (OGC)\n",
    "* Allows tabular geospatial data to be accessed via the Web.\n",
    "* A _feature_ has a _geometry_ (e.g. a point/line/polygon) indicating a geographic location, and a set of properties (e.g. temperature) \n",
    "* WFS allows filtering based on geometry or properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d1270-159f-46da-b6b8-b2162b0401d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "For example, most of the tabular data from the Australian Integrated Marine Observing System (IMOS) is available via WFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250dfb4-fc51-4db7-9088-17bd2bd9e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owslib.wfs import WebFeatureService\n",
    "\n",
    "wfs = WebFeatureService(url=\"https://geoserver-123.aodn.org.au/geoserver/wfs\",\n",
    "                        version=\"1.1.0\")\n",
    "wfs.identification.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41cd03-3895-490a-8621-95a569a53a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dataset is served as a separate \"feature type\":\n",
    "print(f\"There are {len(wfs.contents)} fature types, e.g.\")\n",
    "list(wfs.contents)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08076cde-3bec-4199-932a-61ee43e8df64",
   "metadata": {
    "tags": []
   },
   "source": [
    "For now we'll assume we already know which featuretype we want. It's a dataset containing selected CTD profiles obtained at the National Reference Stations around australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f370e7-a79c-431e-9059-fee45837b0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "typename = 'imos:anmn_ctd_profiles_data'   # Metadata: https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/7b901002-b1dc-46c3-89f2-b4951cedca48\n",
    "wfs.get_schema(typename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02cee3-558e-4515-aaae-c5353f4c3f29",
   "metadata": {},
   "source": [
    "We can read in a subset of the data by specifying a bounding box (in this case near Sydney, Australia).\n",
    "\n",
    "We'll get the result in CSV format so it's easy to read into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef812f4-7db4-4b35-a442-f09b11b0b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xmin, xmax = 151.0, 151.5   # Port Hacking, near Sydney, NSW\n",
    "ymin, ymax = -34.5, -34.0\n",
    "\n",
    "response = wfs.getfeature(typename=typename, bbox=(xmin, ymin, xmax, ymax), outputFormat='csv')\n",
    "df = pd.read_csv(response)\n",
    "response.close()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bcbb6-605c-4aa9-8be1-8370a8d4b44e",
   "metadata": {},
   "source": [
    "We can also filter the data based on the values in specified columns (properties) and ask for only a subset of the columns to be returned. The filters need to be provided in XML format, but the `owslib` library allows us to construct them in a more Pythonic way.\n",
    "\n",
    "Here we select only the profiles associated with the Port Hacking 100m mooring site, and only the data points flagged as \"good data\" by automated quality-control procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb48e6d-077c-4c8a-8e76-81340e469548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owslib.etree import etree\n",
    "from owslib.fes import PropertyIsEqualTo, And\n",
    "\n",
    "filter = And([PropertyIsEqualTo(propertyname=\"site_code\", literal=\"PH100\"),\n",
    "              PropertyIsEqualTo(propertyname=\"PRES_REL_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"TEMP_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"PSAL_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"CPHL_quality_control\", literal=\"1\")\n",
    "             ])\n",
    "filterxml = etree.tostring(filter.toXML(), encoding=\"unicode\")\n",
    "response = wfs.getfeature(typename=typename, filter=filterxml, outputFormat=\"csv\",\n",
    "                          propertyname=[\"TIME\", \"DEPTH\", \"TEMP\", \"PSAL\", \"CPHL\"]\n",
    "                         )\n",
    "df = pd.read_csv(response, parse_dates=[\"TIME\"])\n",
    "response.close()\n",
    "\n",
    "# the server adds a feature ID column we don't really need\n",
    "df.drop(columns='FID', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24705ca-8b5b-42a0-9b68-c482c66bae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews\n",
    "import hvplot.pandas\n",
    "\n",
    "temp_plot = df.hvplot(x=\"TEMP\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False).opts(width=300)\n",
    "psal_plot = df.hvplot(x=\"PSAL\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False).opts(width=300)\n",
    "cphl_plot = df.hvplot(x=\"CPHL\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False).opts(width=300)\n",
    "\n",
    "(temp_plot + psal_plot + cphl_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ab58b-a465-4800-8172-c7b2cb7e4411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract all the temperature measurements at a fixed depth and compare to the timeseries from the mooring \n",
    "comp_depth = 20  # metres\n",
    "\n",
    "df_sub = df[df.DEPTH.round() == comp_depth]\n",
    "ctd_plot = df_sub.hvplot.scatter(x=\"TIME\", y=\"TEMP\", c=\"red\")\n",
    "\n",
    "mooring_plot = ds_mooring.TEMP.sel(DEPTH=comp_depth).hvplot()\n",
    "\n",
    "mooring_plot * ctd_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14889af6-212e-48d1-be86-636cf46a21c1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "source": [
    "Further examples?\n",
    "* Plot timeseries of near-surface values\n",
    "* Plot profile by month of year?\n",
    "* Complute MLD (or read from `nrs_derived_indices_data`) and plot timeseries\n",
    "* Calculate average profile per month of year?\n",
    "* Plot timeseries of various phytoplankton species abundances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bba9a-3fa1-43d0-984d-1532699d6423",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "TODO"
    ]
   },
   "source": [
    "**TODO** Add abstract & metadata link to the example WFS layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae0efa-8466-421d-aace-39f5259cd695",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading files on cloud storage\n",
    "\n",
    "Data files made available to the public on cloud storage such as [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) can be accessed over the web as if they were stored locally. You just need to find the exact URL for each file.\n",
    "\n",
    "In Python, we can access S3 storage in a very similar way to a local filesystem using the `s3fs` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdff28-e958-42a8-8a07-43ebd5583020",
   "metadata": {
    "tags": []
   },
   "source": [
    "For example, all the public data files hosted by the Australian Ocean Data Network are stored in an [S3 bucket](https://www.techtarget.com/searchaws/definition/AWS-bucket) called `imos-data`. You can browse the contents of the bucket and download individual files [here](https://imos-data.aodn.org.au). \n",
    "\n",
    "Below we'll look at a [high-resolution regional SST product](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/a4170ca8-0942-4d13-bdb8-ad4718ce14bb) from IMOS (based on satellite and in-situ observations). This product is a collection of daily gridded NetCDF files covering the Australian region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9e6e6-c345-46bf-a325-1f9e98059303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# List the most recent files available\n",
    "sst_files = s3.ls(\"imos-data/IMOS/SRS/SST/ghrsst/L4/RAMSSA/2023\")\n",
    "sst_files[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285652d9-de6c-48a8-917a-a7447a7c95b3",
   "metadata": {
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "\n",
    "# Open the latest file and look at its contents\n",
    "ds = xr.open_dataset(s3.open(sst_files[-1]))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e72d7b-ad15-4f46-a6e6-0671d8f6ae33",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "# import libraries for plotting geographic data & maps\n",
    "\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from geoviews import opts\n",
    "from cartopy import crs\n",
    "\n",
    "gv.extension('bokeh', 'matplotlib')\n",
    "gv.output(size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e48b0-e0d8-475b-ae1f-17bf90cfe9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot a subset of the dataset around Australia\n",
    "\n",
    "sst_var = 'analysed_sst'\n",
    "gds = gv.Dataset(ds.sel(lat=slice(-50, 0), lon=slice(105, 175)),\n",
    "                 kdims=['lon', 'lat'],\n",
    "                 vdims=[sst_var],\n",
    "                 crs=crs.PlateCarree()  #central_longitude=180)  # this is needed to properly handle lat > 180\n",
    "                )\n",
    "sst_plot = gds.to(gv.Image)\n",
    "sst_plot.opts(cmap='coolwarm', colorbar=True, width=600, height=500, title=ds.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279dce1b-0c3b-4294-9ccc-273a68349a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "It's worth understanding a little about how this works. \n",
    "\n",
    "The above example only makes use of the metadata from the file, one of the 4 data variables, and the `lon` and `lat` coordinates. On a local filesystem, it would be easy to read only these specific parts of the file from disk. \n",
    "\n",
    "However, on cloud storage services like S3 (also called \"object storage\") the basic read/write functions operate on the entire file (object), so at least in the backend, the entire file is read**. If you only need a small subset of a large file, this can be a very inefficient way to get it.\n",
    "\n",
    "** _Note: it is possible to request only a subset of an S3 object to be read, but this is more advanced usage than what we're doing here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e731bc-985f-4d29-8647-8f0e2c476fe7",
   "metadata": {
    "tags": [
     "DRAFT"
    ]
   },
   "source": [
    "For example, if we wanted to plot a timeseries of the above satellite SST product at a given point, we would only need a single value out of each file (corresponding to one point in the timeseries), but the entire file would need to be read each time.\n",
    "\n",
    "Let's try plotting the last 30 days of data for a point East of Tasmania..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac703357-680d-4ada-b536-93f81a3b3daf",
   "metadata": {
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_objs = [s3.open(f) for f in sst_files[-30:]]\n",
    "mds = xr.open_mfdataset(s3_objs, engine=\"h5netcdf\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42039a2-691e-4f9f-b936-f9d252215b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds[sst_var].sel(lat=-42, lon=150, method=\"nearest\").hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89076b-480b-4a04-b7ca-c582786879fb",
   "metadata": {},
   "source": [
    "### Zarr - a cloud-optimised data format\n",
    "\n",
    "Zarr is a relatively new data format specifically developed for efficient access to multi-dimensional data in the cloud. Each dataset is broken up into many smaller files containing \"chunks\" of the data, organised in a standard hierarchy. The metadata are stored in separate files. When reading such a dataset, only the required information is read for each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89598a0d-e160-4e74-b8f4-58164f747c7e",
   "metadata": {},
   "source": [
    "Access a Zarr dataset is simple, as it is very similar to accessing a local NetCDF file with `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aabac-3b27-4a93-8c68-8d8004c719e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66e306b-1fe7-4083-bcb1-429325eae669",
   "metadata": {},
   "source": [
    "### Parquet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa13e5-5f12-4895-936a-3dff24cf4819",
   "metadata": {},
   "source": [
    "## New OGC APIs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc285e-7373-4105-8b57-88b980bfb582",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "TODO"
    ]
   },
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] Add metadata links for datasets used\n",
    "- [ ] Find alternative data sources in other regions (at least US)\n",
    "- [ ] Provide sample data to store on JupyterHub for local access\n",
    "- [ ] Provide clear instructions for participants which data to access!\n",
    "- [ ] Acknowledge previous tutorial and other sources...\n",
    "- [ ] Data acknowledgements / citations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bf046-6fad-4253-b6de-5909871bfe49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OHW ipykernel",
   "language": "python",
   "name": "ohw-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
